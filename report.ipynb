{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Movie Review Sentiment Prediction\n",
    "\n",
    "This report is written with jupyter notebook and converted to pdf, so if you have jupyter installed, you can run the file report.ipynb.\n",
    "\n",
    "Use nltk to tokenize and count the number of each words. For information on installation see README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Use os.listdir to find all file names and then iterate throught them and read each file into a string, omitting linebreaks and apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reviews_dir_pos = 'review_polarity/txt_sentoken/pos'\n",
    "reviews_dir_neg = 'review_polarity/txt_sentoken/neg'\n",
    "pos_reviews = os.listdir(reviews_dir_pos)\n",
    "neg_reviews = os.listdir(reviews_dir_neg)\n",
    "\n",
    "positive_str = []\n",
    "negative_str = []\n",
    "# read in positive reviews\n",
    "for review in pos_reviews:\n",
    "    with open(os.path.join(reviews_dir_pos, review), 'r') as file:\n",
    "        review_str = file.read().replace('\\n', '').replace(\"'\", '')\n",
    "        positive_str.append(review_str)\n",
    "        \n",
    "for review in neg_reviews:\n",
    "    with open(os.path.join(reviews_dir_neg, review), 'r') as file:\n",
    "        review_str = file.read().replace('\\n', '')\n",
    "        negative_str.append(review_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get a list of all words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common 30 words in all reviews are: \n[('the', 76528), ('a', 38100), ('and', 35576), ('of', 34123), ('to', 31937),\n ('is', 25195), ('in', 21822), ('that', 15566), ('it', 14200), ('as', 11378),\n ('with', 10792), ('for', 9961), ('his', 9587), ('this', 9577), ('film', 9196),\n ('s', 9077), ('but', 8634), ('he', 8267), ('i', 8259), ('on', 7382),\n ('are', 6949), ('by', 6261), ('be', 6173), ('one', 5816), ('an', 5744),\n ('movie', 5665), ('not', 5577), ('who', 5548), ('from', 4999), ('at', 4986)]\n============================================\nTotal number of unique words left in the word set: 252\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "mega_str = ''.join(positive_str + negative_str)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "all_count = Counter(tokenizer.tokenize(mega_str))\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=80, compact=True)\n",
    "print('The most common {} words in all reviews are: '.format(30))\n",
    "top_common = all_count.most_common(30)\n",
    "pp.pprint(top_common)\n",
    "\n",
    "# delete top 60 common words\n",
    "for key in list(zip(*top_common))[0]:\n",
    "    del all_count[key]\n",
    "\n",
    "\"\"\"\n",
    "still too many features, cut out least common ones\n",
    "\"\"\"\n",
    "feature_count = {key: value for key, value in all_count.items() if value > 500}\n",
    "\n",
    "print('============================================')\n",
    "print('Total number of unique words left in the word set: {}'.format(len(feature_count)))\n",
    "\n",
    "feature_keys = list(feature_count.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "At this stage, we've got our feature list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pos_tokens = []\n",
    "neg_tokens = []\n",
    "\n",
    "for review in positive_str:\n",
    "    count = dict(Counter(tokenizer.tokenize(review)))\n",
    "    feature_dict = {}\n",
    "    for key in feature_keys:\n",
    "        if key in count:\n",
    "            feature_dict[key] = count[key]\n",
    "        else:\n",
    "            feature_dict[key] = 0\n",
    "    pos_tokens.append(feature_dict)\n",
    "    \n",
    "for review in negative_str:\n",
    "    count = dict(Counter(tokenizer.tokenize(review)))\n",
    "    feature_dict = {}\n",
    "    for key in feature_keys:\n",
    "        if key in count:\n",
    "            feature_dict[key] = count[key]\n",
    "        else:\n",
    "            feature_dict[key] = 0\n",
    "    neg_tokens.append(feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The final step: shuffle the data. and assign the target label to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(pos_tokens)):\n",
    "    pos_tokens[i]['@'] = 1\n",
    "    \n",
    "for i in range(0, len(neg_tokens)):\n",
    "    neg_tokens[i]['@'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 2000, each has 252 features\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "features = pos_tokens + neg_tokens\n",
    "print('Total number of reviews: {}, each has {} features'.format(len(features),\n",
    "                                                                 len(features[0])-1))\n",
    "\n",
    "shuffle(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(0, len(features)):\n",
    "    labels.append(features[i]['@'])\n",
    "\n",
    "for entry in features:\n",
    "    try:\n",
    "        del entry['@']\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training the perceptron\n",
    "\n",
    "The perceptron classifier takes arrays as input, so we need to turn the dictionary into an python list. The training is set to stop after 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from learner.perceptron import Perceptron\n",
    "\n",
    "features_list = [list(entry.values()) for entry in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Average results from 5 fold CV: {'precision': 0.89983335676495457, \"\n \"'accuracy': 0.91649999999999987, 'f-beta': 0.91814742994313003, 'recall': \"\n '0.93833168000005429}')\n"
     ]
    }
   ],
   "source": [
    "clf = Perceptron()\n",
    "score_dict = clf.score(features_list, labels)\n",
    "pp.pprint('Average results from 5 fold CV: {}'.format(score_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "As can be seen from above, the 5 fold CV yielded an average test accuracy of 0.916, and a f-beta score of 0.918 at beta equals to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training Naive Bayes\n",
    "\n",
    "Unlike perceptron classifier, the naive bayes takes dictionaries as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.5, 'accuracy': 0.5, 'f-beta': 0.66617541035360561, 'recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from learner.bayesian_learner import NaiveBayesClassifier\n",
    "\n",
    "clf = NaiveBayesClassifier()\n",
    "clf.train(features[:1500], labels[:1500])\n",
    "print(clf.score(features, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3(tensorflow)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}